// Ignore config parameters in validation

validation {
    ignoreParams = [
        'vsc_calcua_scratchdir',
        'vsc_calcua_apptainer_tmpdir',
        'vsc_calcua_apptainer_cachedir',
        'vsc_calcua_nxf_apptainer_cachedir',
        'vsc_calcua_slurm_scheduling',
        'vsc_calcua_partition',
        'vsc_calcua_get_allocated_cpus_func',
        'vsc_calcua_get_allocated_mem_func'
    ]
}

// Define the scratch directory, which will be used for storing the nextflow
// work directory and for caching apptainer/singularity files.
// Default to /tmp directory if $VSC_SCRATCH scratch env is not available,
// see: https://github.com/nf-core/configs?tab=readme-ov-file#adding-a-new-config
params.vsc_calcua_scratchdir = System.getenv("VSC_SCRATCH") ?: "/tmp"

// Specify the work directory. Can be overwritten via the cli flag `-work-dir`.
workDir = "${params.vsc_calcua_scratchdir}/work"

// Perform work directory cleanup when the run has succesfully completed.
cleanup = true

// Check if environment variables for singularity/apptainer/nextflow cache and tmp dirs are set:
// - APPTAINER_TMPDIR/SINGULARITY_TMPDIR (warn if missing, apptainer defaults to $TMPDIR or /tmp)
// - APPTAINER_CACHEDIR/SINGULARITY_CACHEDIR (exit with error if missing, apptainer would default to $HOME otherwise)
// - NXF_APPTAINER_CACHEDIR/NXF_SINGULARITY_CACHEDIR (warn and set to ${params.vsc_calcua_scratchdir}/apptainer/nextflow_cache if missing)
// Note that only the third env var can be set inside of this config file (cacheDir), because
// the env scope only provides env vars to tasks, not to the launch environment.
// See https://www.nextflow.io/docs/latest/config.html#scope-env

// Define variables outside of conditional scope to make them usable elsewhere
params.vsc_calcua_apptainer_tmpdir = {
    // APPTAINER_TMPDIR/SINGULARITY_TMPDIR environment variable
    def tmpdir = System.getenv("APPTAINER_TMPDIR") ?: System.getenv("SINGULARITY_TMPDIR") ?: null
    if(!tmpdir) {
        // Apptainer defaults to $TMPDIR or /tmp (on the Slurm execution node) if this env var is not set.
        // See https://apptainer.org/docs/user/main/build_env.html#temporary-folders
        tmpdir = System.getenv("TMPDIR") ?: "/tmp"
        System.err.println(
            "\nWARNING: APPTAINER_TMPDIR/SINGULARITY_TMPDIR environment variable was not found." +
            "\nPlease add the line 'export APPTAINER_TMPDIR=\"\${VSC_SCRATCH}/apptainer/tmp\"' " +
            "to your ~/.bashrc file (or set it with sbatch or in your job script)." +
            "\nDefaulting to local $tmpdir on the execution node of the Nextflow head process.\n"
        )
    } else {
        // If set, try to create the tmp directory at the specified location to avoid errors during
        // docker image conversion (note that this only happens when no native singulariry/apptainer
        // images are available):
        //      FATAL:   While making image from oci registry: error fetching image to cache: while
        //      building SIF from layers: unable to create new build: failed to create build parent dir:
        //      stat /scratch/antwerpen/203/vsc20380/apptainer/tmp: no such file or directory
        def tmpdir_file = new File(tmpdir)
        if (! tmpdir_file.exists() ) {
            try {
                tmpdir_file.mkdirs()
            } catch (java.io.IOException e) {
                System.err.println(
                    "\nERROR: Could not create directory at the location specified by APPTAINER_TMPDIR/SINGULARITY_TMPDIR:" +
                    " $tmpdir\nPlease check if this is a valid path to which you have write permission. Exiting...\n" +
                    "Error message: ${e.getMessage()}\n"
                )
                System.exit(1)
            }
        }
    }
    return tmpdir
}.call()


params.vsc_calcua_apptainer_cachedir = {
    // APPTAINER_CACHEDIR/SINGULARITY_CACHEDIR
    def cachedir = System.getenv("APPTAINER_CACHEDIR") ?: System.getenv("SINGULARITY_CACHEDIR") ?: null
    if ( !cachedir ) {
        System.err.println(
            "\nERROR: APPTAINER_CACHEDIR/SINGULARITY_CACHEDIR environment variable was not found." +
            "\nPlease add the line 'export APPTAINER_CACHEDIR=\"\${VSC_SCRATCH}/apptainer/cache\"'" +
            " to your ~/.bashrc file (or set it with sbatch or in your job script).\n" +
            "Using the default storage location of Singularity/Apptainer ~/.apptainer/cache/. " +
            "Read more about why this should be avoided in the VSC docs: " +
            "https://docs.vscentrum.be/software/singularity.html#building-on-vsc-infrastructure\n"
        )
        System.exit(1)
    }
    return cachedir
}.call()

params.vsc_calcua_nxf_apptainer_cachedir = {
    // NXF_APPTAINER_CACHEDIR/NXF_SINGULARITY_CACHEDIR
    def cachedir = System.getenv("NXF_APPTAINER_CACHEDIR") ?: System.getenv("NXF_SINGULARITY_CACHEDIR") ?: null
    if ( !cachedir ) {
        cachedir = "${params.vsc_calcua_scratchdir}/apptainer/nextflow_cache"
        System.err.println(
            "\nWARNING: NXF_APPTAINER_CACHEDIR/NXF_SINGULARITY_CACHEDIR environment variable was not found." +
            "\nPlease add the line 'export NXF_APPTAINER_CACHEDIR=\"\${VSC_SCRATCH}/apptainer/nextflow_cache\"'" +
            " to your ~/.bashrc file (or set it with sbatch or in your job script) to choose the location of" +
            " the Nextflow container image cache.\nDefaulting to $cachedir (instead of the Nextflow work directory).\n")
    }
    return cachedir
}.call()

// Reduce the job submit rate to about 30 per minute, this way the server
// won't be bombarded with jobs.
// Limit queueSize to keep job rate under control and avoid timeouts.
// Set read timeout to the maximum wall time.
// See: https://www.nextflow.io/docs/latest/config.html#scope-executor
executor {
    submitRateLimit = "30/1min"
    queueSize = 20
    exitReadTimeout = "10 min"
}

// Add backoff strategy to catch cluster timeouts and proper symlinks of files in scratch
// to the work directory.
// See: https://www.nextflow.io/docs/latest/config.html#scope-process
process {
    stageInMode = "symlink"
    stageOutMode = "rsync"
    errorStrategy = { sleep(Math.pow(2, task.attempt ?: 1) * 200 as long); return "retry" }
    maxRetries = 3
}

// Specify that apptainer/singularity should be used and where the cache dir will be for the images.
// Singularity is used in favour of apptainer, because currently the apptainer
// variant will pull in (and convert) docker images, instead of using pre-built singularity ones.
// On a system where singularity is defined as an alias for apptainer (as is the case on CalcUA),
// this works out fine and results in pre-built singularity containers being downloaded.
// See https://nf-co.re/docs/usage/installation#pipeline-software
// and https://nf-co.re/tools#how-the-singularity-image-downloads-work
// See https://www.nextflow.io/docs/latest/config.html#scope-singularity
singularity {
    enabled = true
    autoMounts = true
    // See https://www.nextflow.io/docs/latest/singularity.html#singularity-docker-hub
    cacheDir = "${params.vsc_calcua_nxf_apptainer_cachedir}" // Equivalent to setting NXF_APPTAINER_CACHEDIR/NXF_SINGULARITY_CACHEDIR environment variable
}

// Shared profile settings
params.config_profile_description = 'Configuration profile for execution of Nextflow pipelines on the CalcUA VSC HPC..'
params.config_profile_contact = "GitHub: @pmoris - Email: pmoris@itg.be"
params.config_profile_url = "https://docs.vscentrum.be/antwerp/tier2_hardware.html"

// Retrieve name of current partition via Slurm environment variable
params.vsc_calcua_partition =  {
    def partition = System.getenv("SLURM_JOB_PARTITION") ?: null
    // Skip check if host is not CalcUA, to avoid hindering github actions.
    if ( System.getenv("VSC_INSTITUTE") == "antwerpen" ) {
        if(!partition) {
            System.err.println("WARNING: Could not retrieve name of current Slurm partition/queue, defaulting to broadwell")
            partition = "broadwell"
        }
    }
    return partition
}.call()

// Use slurm executor as default, but enable switching to local for single node profile
params.vsc_calcua_slurm_scheduling = true
profiles {
    single_node {
        params.vsc_calcua_slurm_scheduling = false
    }
}

// Dynamic partition/queue selection; adapted from https://nf-co.re/configs/vsc_ugent
// Define profiles for the following partitions:
// - zen2, zen3, zen3_512 (Vaughan)
// - broadwell, broadwell_256 (Leibniz)
// - skylake (Breniac, formerly Hopper)

params.max_memory = params.vsc_calcua_partition == 'zen2' ? params.vsc_calcua_get_allocated_mem_func(240) :
                    params.vsc_calcua_partition == 'zen3' ? params.vsc_calcua_get_allocated_mem_func(240) :
                    params.vsc_calcua_partition == 'zen3_512' ? params.vsc_calcua_get_allocated_mem_func(496) :
                    params.vsc_calcua_partition == 'broadwell' ? params.vsc_calcua_get_allocated_mem_func(112) :
                    params.vsc_calcua_partition == 'broadwell_256' ? params.vsc_calcua_get_allocated_mem_func(240) :
                    params.vsc_calcua_partition == 'skylake' ? params.vsc_calcua_get_allocated_mem_func(176) :
                    null

params.max_cpus =   params.vsc_calcua_partition == 'zen2' ? params.vsc_calcua_get_allocated_cpus_func(64) :
                    params.vsc_calcua_partition == 'zen3' ? params.vsc_calcua_get_allocated_cpus_func(64) :
                    params.vsc_calcua_partition == 'zen3_512' ? params.vsc_calcua_get_allocated_cpus_func(64) :
                    params.vsc_calcua_partition == 'broadwell' ? params.vsc_calcua_get_allocated_cpus_func(28) :
                    params.vsc_calcua_partition == 'broadwell_256' ? params.vsc_calcua_get_allocated_cpus_func(28) :
                    params.vsc_calcua_partition == 'skylake' ? params.vsc_calcua_get_allocated_cpus_func(28) :
                    null

params.max_time =   params.vsc_calcua_partition == 'skylake' ? 7.day : 3.day

process.resourceLimits = [
    memory: params.max_memory,
    cpus: params.max_cpus,
    time: params.max_time
]
process.executor = params.vsc_calcua_slurm_scheduling ? "slurm" : "local"
process.queue = params.vsc_calcua_partition

// Define functions to fetch the available CPUs and memory of the current execution node.
// Only used when the single_node / local execution profile is activated.
// Allows cpu and memory thresholds to be set dynamic based on the available hardware as reported
// by Slurm. Can be supplied with a default return value, which should be set to the
// recommended thresholds for that particular partition's node types.
params.vsc_calcua_get_allocated_cpus_func = { int node_max_cpu ->
    def int max_cpus = node_max_cpu
    if(params.vsc_calcua_slurm_scheduling) {
        max_cpus = System.getenv("SLURM_CPUS_PER_TASK") ?: System.getenv("SLURM_JOB_CPUS_PER_NODE") ?: node_max_cpu
    }
    return max_cpus.toInteger()
}

params.vsc_calcua_get_allocated_mem_func = { int node_max_mem ->
    // default to max memory of node per partition type
    def int max_mem = node_max_mem

    if(params.vsc_calcua_slurm_scheduling) {
        // grab environment variables with memory and cpu info
        def mem_per_cpu = System.getenv("SLURM_MEM_PER_CPU")
        def mem_per_node = System.getenv("SLURM_MEM_PER_NODE")
        def cpus_per_task = System.getenv("SLURM_CPUS_PER_TASK") ?: System.getenv("SLURM_JOB_CPUS_PER_NODE")

        // Check if memory was requested per cpu and the number of cpus was also set
        if ( mem_per_cpu && cpus_per_task ) {
            max_mem = mem_per_cpu.toInteger() / 1000 * cpus_per_task.toInteger()
        }
        // Check if total/node memory was requested instead
        else if ( mem_per_node ) {
            max_mem = mem_per_node.toInteger() / 1000
        }
    }

    // return in expected GB string format
    return "${max_mem}.GB"
}
