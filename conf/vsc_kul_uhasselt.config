// Ignore configuration parameters in validation
validation {
    ignoreParams = [
        'vsc_kul_hasselt_scratchdir',
        'vsc_kul_hasselt_tier2_project',
        'vsc_kul_hasselt_available_queues',
        'vsc_kul_hasselt_publish_location',
        'vsc_kul_hasselt_time_threshold',
        'vsc_kul_hasselt_memory_threshold_genius',
        'vsc_kul_hasselt_memory_threshold_wice',
        'vsc_kul_hasselt_limit_task_time_func',
        'vsc_kul_hasselt_determine_genius_queue_func',
        'vsc_kul_hasselt_determine_genius_gpu_queue_func',
        'vsc_kul_hasselt_determine_wice_queue_func',
        'vsc_kul_hasselt_determine_wice_gpu_queue_func'
    ]
}

// Default to /tmp directory if $VSC_SCRATCH scratch env is not available,
// see: https://github.com/nf-core/configs?tab=readme-ov-file#adding-a-new-config
params.vsc_kul_hasselt_scratchdir       = System.getenv("VSC_SCRATCH") ?: "/tmp"
params.vsc_kul_hasselt_tier2_project    = System.getenv("SLURM_ACCOUNT") ?: null
params.vsc_kul_hasselt_available_queues = (System.getenv("VSC_DEDICATED_QUEUES") ?: "").toString().split(',')

// Perform work directory cleanup when the run has succesfully completed
// cleanup = true

// Reduce the job submit rate to about 50 per minute, this way the server won't be bombarded with jobs
// Limit queueSize to keep job rate under control and avoid timeouts
executor {
    submitRateLimit = '50/1min'
    queueSize = 50
    exitReadTimeout = "10min"
}

// Add backoff strategy to catch cluster timeouts and proper symlinks of files in scratch to the work directory
process {
    executor      = 'slurm'
    stageInMode   = "symlink"
    stageOutMode  = "rsync"
    errorStrategy = { sleep(Math.pow(2, task.attempt ?: 1) * 200 as long); return 'retry' }
    maxRetries    = 3
}

// Specify that singularity should be used and where the cache dir will be for the images
singularity {
    enabled     = true
    autoMounts  = true
    cacheDir    = "${params.vsc_kul_hasselt_scratchdir}/.singularity"
    pullTimeout = "30 min"
}

params {
    config_profile_contact     = 'GitHub: @Joon-Klaps - Email: joon.klaps@kuleuven.be'
    config_profile_url         = 'https://docs.vscentrum.be/en/latest/index.html'
}

params.vsc_kul_hasselt_publish_location = params.get('outdir', "$launchDir")
params.vsc_kul_hasselt_co2_timestamp = params.containsKey('trace_report_suffix') ? params.trace_report_suffix : new java.util.Date().format('yyyy-MM-dd_HH-mm-ss')

co2footprint {
    traceFile = "${params.vsc_kul_hasselt_publish_location}/pipeline_info/co2footprint_trace_${params.vsc_kul_hasselt_co2_timestamp}.txt"
    summaryFile = "${params.vsc_kul_hasselt_publish_location}/pipeline_info/co2footprint_summary_${params.vsc_kul_hasselt_co2_timestamp}.txt"
    reportFile = "${params.vsc_kul_hasselt_publish_location}/pipeline_info/co2footprint_report_${params.vsc_kul_hasselt_co2_timestamp}.html"
    location = 'BE'
    //pue = 1.33                                   // replace with PUE of your data center
    machineType = 'compute cluster'             // set to 'compute cluster', 'local', or 'cloud'
}

env {
    APPTAINER_TMPDIR="${params.vsc_kul_hasselt_scratchdir}/.apptainer/tmp"
    APPTAINER_CACHEDIR="${params.vsc_kul_hasselt_scratchdir}/.apptainer/cache"
}

// AWS maximum retries for errors (This way the pipeline doesn't fail if the download fails one time)
aws {
    client {
        maxErrorRetry = 3
    }
}

/*
 * Queue Selection Utility Functions for HPC Environments
 * ==================================================
 * This module provides functions to determine appropriate HPC queues based on task requirements
 * for both GENIUS and WICE clusters.
 */

/*
 * Constants:
 * ----------
 * params.vsc_kul_hasselt_time_threshold: 72 hours - Threshold for determining long-running jobs
 * params.vsc_kul_hasselt_memory_threshold_genius: 175GB - Memory threshold for bigmem queues
 * params.vsc_kul_hasselt_memory_threshold_wice: 239GB - Memory threshold for high-memory queues
*/
params.vsc_kul_hasselt_time_threshold = 72.h
params.vsc_kul_hasselt_memory_threshold_genius = 175.GB
params.vsc_kul_hasselt_memory_threshold_wice = 239.GB

/*
 * ---------
 * Functions:
 * ----------
 * These functions are designed to select the appropriate HPC queues of
 * VSC_KUL_UHASSELT based on task requirements. They handle both standard
 * and GPU queues, considering memory requirements, execution time, and
 * queue availability.
*/

/*
 * params.vsc_kul_hasselt_limit_task_time_func(time, maxTime)
 *     Ensures task time doesn't exceed the maximum allowed time
 *     @param time Current task time
 *     @param maxTime Maximum allowed time
 *     @return Limited task time
*/
params.vsc_kul_hasselt_limit_task_time_func = { time, maxTime ->
    return time > maxTime ? maxTime : time
}

/*
 * params.vsc_kul_hasselt_determine_genius_queue_func(task)
 *     Selects appropriate CPU queue for GENIUS cluster
 *     @param task Nextflow task object containing memory and time requirements
 *     @return Queue name based on task requirements
*/
params.vsc_kul_hasselt_determine_genius_queue_func = { task ->
    def isHighMemory = task.memory >= params.vsc_kul_hasselt_memory_threshold_genius
    def isLongRunning = task.time >= params.vsc_kul_hasselt_time_threshold
    def hasDedicatedBigmem = params.vsc_kul_hasselt_available_queues.contains('dedicated_big_bigmem')

    if (isHighMemory) {
        return isLongRunning ?
            (hasDedicatedBigmem ? 'dedicated_big_bigmem' : 'bigmem_long') :
            'bigmem'
    }

    return isLongRunning ? 'batch_long' : 'batch'
}

/*
 * params.vsc_kul_hasselt_determine_genius_gpu_queue_func(task)
 *     Selects appropriate GPU queue for GENIUS cluster
 *     @param task Nextflow task object containing memory and time requirements
 *     @return GPU queue name based on task requirements
*/
params.vsc_kul_hasselt_determine_genius_gpu_queue_func = { task ->
    def isHighMemory = task.memory >= params.vsc_kul_hasselt_memory_threshold_genius
    def isLongRunning = task.time >= params.vsc_kul_hasselt_time_threshold
    def hasDedicatedGpu = params.vsc_kul_hasselt_available_queues.contains('dedicated_rega_gpu')
    def hasAmdGpu = params.vsc_kul_hasselt_available_queues.contains('amd')

    if (isHighMemory) {
        return isLongRunning ? 'gpu_v100_long' : 'gpu_v100'
    }

    if (isLongRunning) {
        if (hasDedicatedGpu) return 'dedicated_rega_gpu'
        if (hasAmdGpu) return 'amd_long'
        return 'gpu_p100_long'
    }

    return hasAmdGpu ? 'amd' : 'gpu_p100'
}

/*
 * params.vsc_kul_hasselt_determine_wice_queue_func(task)
 *     Selects appropriate CPU queue for WICE cluster
 *     @param task Nextflow task object containing memory and time requirements
 *     @return Queue name based on task requirements and availability
*/
params.vsc_kul_hasselt_determine_wice_queue_func = { task ->
    def isHighMemory = task.memory >= params.vsc_kul_hasselt_memory_threshold_wice
    def isLongRunning = task.time >= params.vsc_kul_hasselt_time_threshold
    def hasDedicatedQueue = params.vsc_kul_hasselt_available_queues.contains('dedicated_big_bigmem')

    if (isHighMemory) {
        if (isLongRunning && hasDedicatedQueue) {
            return 'dedicated_big_bigmem'
        }
        task.time = params.vsc_kul_hasselt_limit_task_time_func(task.time, params.vsc_kul_hasselt_time_threshold)
        return 'bigmem,hugemem'
    }

    return isLongRunning ?
        'batch_long,batch_icelake_long,batch_sapphirerapids_long' :
        'batch,batch_sapphirerapids,batch_icelake'
}

/*
 * params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task)
 *     Selects appropriate GPU queue for WICE cluster
 *     @param task Nextflow task object containing memory and time requirements
 *     @return GPU queue name based on task requirements
*/
params.vsc_kul_hasselt_determine_wice_gpu_queue_func = { task ->
    def isHighMemory = task.memory >= params.vsc_kul_hasselt_memory_threshold_wice
    def isLongRunning = task.time >= params.vsc_kul_hasselt_time_threshold
    def hasDedicatedQueue = isHighMemory ?
        params.vsc_kul_hasselt_available_queues.contains('dedicated_big_gpu_h100') :
        params.vsc_kul_hasselt_available_queues.contains('dedicated_big_gpu')

    if (isLongRunning && !hasDedicatedQueue) {
        task.time = params.vsc_kul_hasselt_limit_task_time_func(task.time, params.vsc_kul_hasselt_time_threshold)
    }

    if (isHighMemory) {
        return (isLongRunning && hasDedicatedQueue) ? 'dedicated_big_gpu_h100' : 'gpu_h100'
    }

    return (isLongRunning && hasDedicatedQueue) ? 'dedicated_big_gpu' : 'gpu_a100,gpu'
}

/*
 * ========
 * Profiles
 * ========
    * These profiles define the resource limits, queue selection, and cluster options
    * for WICE and GENIUS clusters. They also include GPU-specific configurations.
    * Details of the resource limits can be found in for genius at
    * https://docs.vscentrum.be/leuven/tier2_hardware/genius_hardware.html
    * and for wice at https://docs.vscentrum.be/leuven/tier2_hardware/wice_hardware.html
*/

// Define profiles for each cluster
profiles {
    genius {
        params.config_profile_description = 'genius profile for use on the genius cluster of the VSC HPC.'

        process {
            // 768 - 65 so 65GB for overhead, max is 720000MB
            resourceLimits = [ memory: 703.GB, cpus: 36, time: 168.h ]
            beforeScript   = { 'module load cluster/genius/' + params.vsc_kul_hasselt_determine_genius_queue_func(task).toString().split(',')[0] }
            queue          = { params.vsc_kul_hasselt_determine_genius_queue_func(task) }
            clusterOptions = {
                params.vsc_kul_hasselt_determine_genius_queue_func(task) =~ /dedicated/ ?
                    "--clusters=genius --account=lp_big_genius_cpu" :
                    "--clusters=genius --account=${params.vsc_kul_hasselt_tier2_project}"
            }

            withLabel: '.*gpu.*'{
                resourceLimits         = [ memory: 703.GB, cpus: 36 , time: 168.h ]
                beforeScript           = { 'module load cluster/genius/' + params.vsc_kul_hasselt_determine_genius_gpu_queue_func(task).toString().split(',')[0] }
                containerOptions       = '--containall --cleanenv --nv'
                queue                  = { params.vsc_kul_hasselt_determine_genius_gpu_queue_func(task) }
                clusterOptions         = {
                    def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/9) as int)
                    "--gres=gpu:${gpus} --clusters=genius --account=${params.vsc_kul_hasselt_tier2_project}"
                }
            }
        }
    }

    genius_gpu {
        params.config_profile_description = 'genius_gpu profile for use on the genius cluster of the VSC HPC.'
        apptainer.runOptions              = '--containall --cleanenv --nv'
        singularity.runOptions            = '--containall --cleanenv --nv'

        process {
            // 768 - 65 so 65GB for overhead, max is 720000MB
            resourceLimits = [ memory: 703.GB, cpus: 36, time: 168.h]
            beforeScript   = { 'module load cluster/genius/' + params.vsc_kul_hasselt_determine_genius_gpu_queue_func(task).toString().split(',')[0] }
            queue          = { params.vsc_kul_hasselt_determine_genius_gpu_queue_func(task) }
            clusterOptions = {
                def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/9) as int)
                "--gres=gpu:${gpus} --clusters=genius --account=${params.vsc_kul_hasselt_tier2_project}"
            }
        }
    }

    wice {
        params.config_profile_description = 'wice profile for use on the Wice cluster of the VSC HPC.'

        process {
            // max is 2016000
            resourceLimits = [ memory: 1968.GB, cpus: 72, time: 168.h ]
            beforeScript   = { 'module load cluster/wice/' + params.vsc_kul_hasselt_determine_wice_queue_func(task).toString().split(',')[0] }
            queue          = { params.vsc_kul_hasselt_determine_wice_queue_func(task) }
            clusterOptions = {
                params.vsc_kul_hasselt_determine_wice_queue_func(task) =~ /dedicated/ ?
                    "--clusters=wice --account=lp_big_wice_cpu" :
                    "--clusters=wice --account=${params.vsc_kul_hasselt_tier2_project}"
            }

            withLabel: '.*gpu.*' {
                resourceLimits         = [ memory: 703.GB, cpus: 64, time: 168.h ]
                containerOptions       = '--containall --cleanenv --nv'
                beforeScript           = { 'module load cluster/wice/' + params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task).toString().split(',')[0] }
                queue                  = { params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task) }
                clusterOptions         = {
                    def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/16) as int)
                    def queueValue = params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task)
                    queueValue =~ /dedicated_big_gpu_h100/ ? "--clusters=wice --account=lp_big_wice_gpu_h100 --gres=gpu:${gpus}" :
                    queueValue =~ /dedicated_big_gpu/ ? "--clusters=wice --account=lp_big_wice_gpu --gres=gpu:${gpus}" :
                    "--clusters=wice --account=${params.vsc_kul_hasselt_tier2_project} --gres=gpu:${gpus}"
                }
            }
        }
    }

    wice_gpu {
        params.config_profile_description = 'wice_gpu profile for use on the Wice cluster of the VSC HPC.'
        apptainer.runOptions              = '--containall --cleanenv --nv'
        singularity.runOptions            = '--containall --cleanenv --nv'

        process {
            // 768 - 65 so 65GB for overhead, max is 720000MB
            beforeScript   = { 'module load cluster/wice/' + params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task).toString().split(',')[0] }
            resourceLimits = [ memory: 703.GB, cpus: 64, time: 168.h ]
            queue          = { params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task) }
            clusterOptions = {
                def gpus = task.accelerator?.request ?: Math.max(1, Math.floor((task.cpus ?:1)/16) as int)
                def queueValue = params.vsc_kul_hasselt_determine_wice_gpu_queue_func(task)
                queueValue =~ /dedicated_big_gpu_h100/ ? "--clusters=wice --account=lp_big_wice_gpu_h100 --gres=gpu:${gpus}" :
                queueValue =~ /dedicated_big_gpu/ ? "--clusters=wice --account=lp_big_wice_gpu --gres=gpu:${gpus}" :
                "--clusters=wice --account=${params.vsc_kul_hasselt_tier2_project} --gres=gpu:${gpus}"
            }
        }
    }

    superdome {
        params.config_profile_description = 'superdome profile for use on the genius cluster of the VSC HPC.'

        process {
            clusterOptions = {"--clusters=genius --account=${params.vsc_kul_hasselt_tier2_project}"}
            beforeScript   = 'module load cluster/genius/superdome'
            // 6000 - 228 so 228GB for overhead, max is 5910888MB
            resourceLimits = [ memory: 5772.GB, cpus: 14, time: 168.h]

            queue = { task.time <= 72.h ? 'superdome' : 'superdome_long' }
        }
    }
}


